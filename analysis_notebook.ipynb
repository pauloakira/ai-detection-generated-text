{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import utils\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download NLTK resources\n",
    "utils.checkForNLTKResources()\n",
    "\n",
    "trainDataset = \"_data/train_essays.csv\"\n",
    "testDataset = \"_data/test_essays.csv\"\n",
    "promtDataset = \"_data/train_prompts.csv\"\n",
    "\n",
    "trainDf = pd.read_csv(trainDataset)\n",
    "testDf = pd.read_csv(testDataset)\n",
    "promptDf = pd.read_csv(promtDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------trainDf-------\")\n",
    "print(trainDf.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--------testDf-------\")\n",
    "print(testDf.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--------promptDf-------\")\n",
    "print(promptDf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt id distribution\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.countplot(x='prompt_id', data=trainDf)\n",
    "plt.title('Prompt ID Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target distribution\n",
    "generated = trainDf['generated'].value_counts().to_dict()\n",
    "print(f\"Human: {generated[0]}\")\n",
    "print(f\"AI: {generated[1]}\")\n",
    "sns.countplot(x='generated', data=trainDf)\n",
    "plt.title('Target Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external datasets to complete the training set\n",
    "externalLLMGeneratedTextsDf = pd.read_csv(\"_data/LLM-Mistral-7B-Instruct-texts/Mistral7B_CME_v7.csv\")\n",
    "print(\"------LLM Mistral 7B Dataset------\")\n",
    "print(externalLLMGeneratedTextsDf.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"------LLM Mistral 7B Disitrbution------\")\n",
    "print(externalLLMGeneratedTextsDf['prompt_name'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "promptNames = promptDf['prompt_name'].to_list()\n",
    "externalLLMDf = externalLLMGeneratedTextsDf[externalLLMGeneratedTextsDf['prompt_name'].isin(promptNames)]\n",
    "externalLLMDf.loc[:,'generated'] = 1\n",
    "promptId_map_dict = {2: 0 , 12: 1} # matching prompt_id columns with the train dataset\n",
    "externalLLMDf[\"prompt_id\"] = externalLLMDf[\"prompt_id\"].map(promptId_map_dict)\n",
    "externalLLMDf = externalLLMDf.drop(columns=['prompt_name'])\n",
    "print(\"------External Dataset------\")\n",
    "print(externalLLMDf.info())\n",
    "print(\"\\n\")\n",
    "print(\"------External Dataset Distribution------\")\n",
    "print(externalLLMDf['prompt_id'].value_counts())\n",
    "\n",
    "# prepare train dataset to be concatenated with the external dataset\n",
    "trainDf = trainDf.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the traind dataset with the external LLM dataset\n",
    "newTrainDf = pd.concat([trainDf, externalLLMDf], ignore_index=True)\n",
    "print(\"------Concatenated Dataset------\")\n",
    "print(newTrainDf['generated'].info())\n",
    "print(\"\\n\")\n",
    "print(\"------Concatenated Dataset Generated Distribution------\")\n",
    "print(newTrainDf['generated'].value_counts())\n",
    "sns.countplot(x='generated', data=newTrainDf)\n",
    "plt.title('Target Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if there are duplicates\n",
    "print(f\"Before dropping duplicates: {len(newTrainDf)}\")\n",
    "newTrainDf = newTrainDf.drop_duplicates(subset=['text'])\n",
    "print(f\"After dropping duplicates: {len(newTrainDf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textSample = newTrainDf['text'].iloc[2]\n",
    "cleanedTextSample = utils.cleanText(textSample, True)\n",
    "print(cleanedTextSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textSample = newTrainDf['text'].iloc[2]\n",
    "cleanedTextSample = utils.cleanText(textSample, False)\n",
    "print(cleanedTextSample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
